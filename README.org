* A GPT-Powered Conversational Read Eval Print Loop for Books&Articles
**TODO gif or youtube example

* To Run
****  - TODO gifs or youtube playlist here
****  - if windows then install windows subsystem for linux 
****  - https://nixos.org/download.html (select appropriate operating system from siderbar)
****  - git clone https://github.com/NotBrianZach/bzabook2aquiz.git
****  - cd bzabook2aquiz
****  - nix-shell (flake is work in progress) todo check if mac and use darwin nixpkgs
****  - make a pull request implementing pretty much all the features this project currently needs to be a nice user experience
****  - npm install
****  - OPENAI_API_KEY=$OPENAI_API_KEY ./bza.mjs -f path_2_ur_pdf_here.pdf
****  - get $OPEN_API_KEY key here if u dont have https://platform.openai.com/account/api-keys
****  - open an issue detailing why doesnt work
****  - ELITE HACKER: possibly move readingList.json into another directory under source control or cloud backup and create a symlink here pointing to it

** How to read Event Loop&Workflows
**** numbers are steps in the event loop, letters are steps in a given workflow (which modifies event loop)
**** a.1 -> step a happens before step 1, but after step 0
**** 1.a -> step 1 happens before step a
**** 1.b -> step 1 happens before step a, step b happens after step a
**** 1-a -> step 1 happens concurrently/asynchronously with step a

** Event Loop: Giving Gpt3 Short&Long Term Memory 
*** 1.IF readingList.json has an entry for bookName, load title&synopsis&rollingSummary from there
*** ELSE try to grab title w/gpt3 from first chunkSize pages, query user to validate 
**** C=confirm gpt3 grabbed a valid title, 
**** any other input is assumed to be the title,
*** then query user for page range of table of contents 
**** if input matches regex ([0-9]*-[0-9]*), grab table of contents from that page range, synopsize with gpt3 and validate with user
**** else ask user input synopsis, confirm they entered it correctly 
*** finally set rollingSummary=empty string
*** 2. IF readingList.json has an entry for bookName, load pageNumber&chunkSize from there, 
*** ELSE set pageNumber&chunkSize from commandline parameters or defaults (0,2)
*** 3. DO pageChunkSummary=queryGPT(beforeContext+synopsis+title+rollingSummary+pages[pageNumber:pageNumber+chunkSize]+afterContext) 
*** 4. rollingSummary=queryGPT3(synopsis+summary) 
*** 5. WHILE (pageNumber < bookLength), set pageNumber=pageNumber+chunkSize, jump back to 3. else continue to 6.
*** 6. call onExit method (cleanup)

** Quiz Workflow: 
**** 1.a. generate quiz,
**** 2.a. display summary of pages[pageNumber:pageNumber+chunkSize] and quiz to the user, record user answer to quiz
**** 6.a parting thoughts from gpt3, record a log of all the summaries and quizzes

** Quiz&Answer Workflow:
**** 1.a. generate quiz,
**** 2.a. display summary of pages[pageNumber:pageNumber+chunkSize] and quiz to the user, record user answer to quiz
**** 2.b. gpt attempts to answer the quiz prints answers,
***** query user-> R for user reply to answers, on other input continue
**** 6.a parting thoughts from gpt3, record a log of all the summaries and quizzes

** Query Workflow: 
**** 1.a query user for question, 
**** 1.b gpt3 answer user query,  
***** query user
****** C=continue to next page,
****** Q=ask another question, repeat 1.b
****** r=query gpt3 w/user reply on question answer,
****** A= append next query input to gpt query at the start of each chunk
*******  "tell a joke about the following text\n" 
****** B= prepend next query input to gpt query at the start of each chunk
*******  "\ntell another joke about the above text that ties into the first joke" 
**** 6.a parting thoughts from gpt3, record a log of all questions&answers

** Rewrite Workflow: 
**** 1.a ask user for character (e.g. socrates) (any string will be accepted)
**** 2.a read pages, rewrite in characters voice

** Optional Toggles (TODO): 
*** Narration: use ? https://github.com/coqui-ai/TTS ? to generate voice to narrate gpt response&queries to user
*** Voice Dictation: use ?talon? to allow voice input?

* Reading List Utility (bzaUtil.sh)

store path to pdf and relevant executable to read it

backup&rotate logs

switch between query or quiz mode without losing page context using logs

* Design decisions

pdf-extract introduces a bunch of binary dependencies relative to
alternative libraries but we want those because they enable ocr on the subset of pdfs
that don't cleanly delineate text (and I am guessing they are fast hopefully)

also it would be nice to use other binary dependencies that can read pdfs or other types of file
from the command line (and have the option to pass in e.g. the current pagenumber)

* Naming

the naive/correct pronounciation sounds like pizza, which is typically
sliced into pieces just like we are chunking up books. Book pizza.

bza is also my initials. #branding

and bza is a short three letter word which is not too overloaded and can be invoked easily on the command line.

finally, book starts with B, quiz ends with Z and A is A. so it's like an anagram of some of the letters.

makes total sense.

[[bzatime.jpg]]

* Inspiration

i have kept, for a couple years, a reading list with commands like

"""

# 0-
ebook-viewer ~/media/books/TheDividedSelf2010.epub --open-at 59

# 0-
xpdf ~/media/books/tcp_ip_networkadministration_3rdedition.pdf 50 -z 200

xpdf ~/media/books/LinuxProgrammingInterface2010.pdf

"""

in a file in my /home/$user/media directory so i could read books from command line and record current position

i had also been looking for technically inclined book club without luck (well i didnt try super hard) 

a thought had been bubbling in my head that I wanted to read books alongside gpt3,

i had previously spent quite some time trying to make multi player choose your own adventure novels a thing (and maybe still plan to?)

i really thought, and think, as a massive wordcel, that computers have a vast potential to create new narrative structures

then i saw this reddit post

https://www.reddit.com/r/singularity/comments/11ho23y/first_post_in_reddit_mistakely_used_a_text_post/

and a within a couple minutes, after some good ole reddit arguing, i started writing this

** Pushdown Large Language Models

a final thought, about fundamental models of computation

the theoretical taxonomy of computation looks like this

finite state machines -> have subset of functionality of -> context free grammars -> have subset of functionality of -> turing machines

traditional narratives are simple finite state machines at the level of pages

most choose your own adventure novels are also finite state machines, though they have a bit more structure since they are not purely sequential

the way I wanted to implement multiplayer choose your own adventure novels,

i believe they would have been more akin to a push down automata, or context free grammar,

since the story would maintain a list of invalidated edges (which could also be thought of as a unique class of "intermediate" node that dont branch),

and transitions between nodes could change the choices available to other players

i think there is a similar analogy going on here.

reddit user SignificanceMassive3's diagram displays a "context free" or "pushdown" large language model (ignore the fact the diagram has two stacks and is ?probably? technically turing complete, we don't push to our long term context after we define it, well, mostly... Look buddy we are operationally a pushdown automata!)
[[PushDownLLM.png]]

which, much like a regular expression is suitable for matching patterns in text, a "push down llm" is suitable for the task of reading along with longer form text 
